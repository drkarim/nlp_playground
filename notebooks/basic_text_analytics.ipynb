{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "#%% \n", 
        "#Loading NLTK\n", 
        "import nltk\n", 
        "from nltk.tokenize import sent_tokenize\n", 
        "from nltk.tokenize import word_tokenize\n", 
        "from nltk.probability import FreqDist\n", 
        "from nltk.corpus import stopwords\n", 
        "\n", 
        "\n", 
        "text=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\n", 
        "The sky is pinkish-blue. You shouldn't eat cardboard\"\"\"\n", 
        "\n", 
        "#%% \n", 
        "# Sentence tokeniser \n", 
        "tokenized_text=sent_tokenize(text)\n", 
        "\n", 
        "# word tokeniser \n", 
        "tokenized_word=word_tokenize(text)\n", 
        "\n", 
        "#%% \n", 
        "# Frequency distribution \n", 
        "fdist = FreqDist(tokenized_word)\n", 
        "\n", 
        "# most common word\n", 
        "fdist.most_common(2)\n", 
        "\n", 
        "#%% \n", 
        "# Frequency Distribution Plot\n", 
        "import matplotlib.pyplot as plt\n", 
        "fdist.plot(30,cumulative=False)\n", 
        "plt.show()\n", 
        "\n", 
        "#%%\n", 
        "\n", 
        "# Stop words \n", 
        "stop_words=set(stopwords.words(\"english\"))\n", 
        "print(stop_words)\n", 
        "\n", 
        "#%% \n", 
        "# filtering stop-words \n", 
        "filtered_sent=[]\n", 
        "for w in tokenized_word:\n", 
        "    if w.lower() not in stop_words:\n", 
        "        filtered_sent.append(w)\n", 
        "print(\"Tokenized Sentence:\",tokenized_word)\n", 
        "print(\"Filterd Sentence:\",filtered_sent)\n", 
        "\n", 
        "#%% \n", 
        "# Stemming\n", 
        "from nltk.stem import PorterStemmer\n", 
        "from nltk.tokenize import sent_tokenize, word_tokenize\n", 
        "\n", 
        "ps = PorterStemmer()\n", 
        "\n", 
        "stemmed_words=[]\n", 
        "for w in filtered_sent:\n", 
        "    stemmed_words.append(ps.stem(w))\n", 
        "\n", 
        "print(\"Filtered Sentence:\",filtered_sent)\n", 
        "print(\"Stemmed Sentence:\",stemmed_words)\n", 
        "\n", 
        "#%% \n", 
        "#Lexicon Normalization\n", 
        "#performing stemming and Lemmatization\n", 
        "from nltk.stem.wordnet import WordNetLemmatizer\n", 
        "lem = WordNetLemmatizer()\n", 
        "\n", 
        "from nltk.stem.porter import PorterStemmer\n", 
        "stem = PorterStemmer()\n", 
        "\n", 
        "words = [\"catching\", \"flying\", \"truncate\"] \n", 
        "words_lemmas = [] \n", 
        "words_stems = []\n", 
        "for w in words: \n", 
        "    words_lemmas.append(lem.lemmatize(w,pos=\"v\"))\n", 
        "    words_stems.append(stem.stem(w))\n", 
        "\n", 
        "print(\"lemma = \",words_lemmas)\n", 
        "print(\"stemmer = \",words_stems)\n", 
        "\n", 
        "\n", 
        "#%%\n", 
        "sent = \"Albert Einstein was born in Ulm, Germany in 1879.\"\n", 
        "tokens=nltk.word_tokenize(sent)\n", 
        "#print(tokens)\n", 
        "nltk.pos_tag(tokens)\n", 
        "\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}